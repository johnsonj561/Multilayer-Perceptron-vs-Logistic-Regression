### Multilayer Perceptron vs Logistic Regression

Logistic Regression and Multilayer Perceptron (MLP) classifiers are trained and evaluated with WEKA using fit and test data sets describing software modules. The data set instances contain 8 independent attributes which describe software module metrics, and a 9th dependent attribute which labels the software module as fault prone or non-fault prone. Both Logistic Regression and MLP classifiers are first evaluated using 10 fold cross-validation against a fit data set of 188 instances, and then evaluated using a separate test data set of 94 instances. For Logistic Regression, WEKA’s cost sensitive classifier is used to vary the cost ratio and identify an ideal model during the cross-validation phase. The selected Logistic Regression model’s test data classification results are then compared to the results produced by the Multilayer Perceptron.

The Logistic Regression model performed the best considering our goal of reducing Type II error rate, classifying fewer fault prone software modules as non-fault prone (Type II error). The Logistic Regression model had slightly more Type I misclassifications, but we are mostly interested in reducing Type II error rates. The MLP model actually had fewer Type I misclassifications, reducing from 15.2% down to 13.6%, but there was an increase in Type II misclassifications from 21.4% to 25%. This Type I vs Type II error rate tradeoff is expected.

The difference in classification results is small, just a 2 - 4 % difference in Type I and Type II error rates between the two classifiers. The fact that the MLP was given a default cost ratio of 1 and the performance was very similar to the Logistic Regression model which required cost sensitive classification is a significant one. The MLP was able to handle and learn the imbalanced data set (roughly 30% positive instances) without any additional work.

It should also be noted that mostly default MLP parameters were used for this assignment. I predict that tuning the MLP classifier can produce better results and potentially outperform the Logistic Regression model. Cost ratio can be adjusted to give Type II misclassifications a higher weight, the learning rate can be adjusted to improve the gradient descent optimization process and avoid local minima, and the number of hidden layers and nodes can be adjusted to identify a more appropriate neural network architecture for the problem at hand. I also predict that an increase in the training data set size will produce a more accurate model. Multilayer Perceptron learners are powerful, but can suffer from overfitting if the network is larger than what is required for the underlying data structure.

